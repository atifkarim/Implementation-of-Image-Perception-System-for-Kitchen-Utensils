{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "import glob, os\n",
    "from skimage import io, color, exposure, transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.layers import Input, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir = '/home/atif/machine_learning_stuff/ml_image/copy_image/train'\n",
    "# validation_dir = '/home/atif/machine_learning_stuff/ml_image/copy_image/validation'\n",
    "\n",
    "train_dir = '/home/atif/machine_learning_stuff/ml_image/train_image_AI'\n",
    "validation_dir = '/home/atif/machine_learning_stuff/ml_image/validation_image_AI'\n",
    "path_pre_trained_model = '/home/atif/machine_learning_stuff/model_file_keras/'\n",
    "IMG_SIZE = 48\n",
    "IMG_depth = 3 # for RGB 3, for B&W it will be 1\n",
    "NUM_CLASSES = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All layers are freezed. Same as pre trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# Download the VGG 16 model . Check here https://github.com/keras-team/keras/issues/5257#issuecomment-314628173 .\n",
    "# with TF backend maybe not possible to use VGG 16 with channel first\n",
    "# vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_depth,IMG_SIZE,IMG_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the VGG model\n",
    "loaded_vgg = VGG16(weights=path_pre_trained_model+'vgg_top_48_shape/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_shape=(IMG_SIZE,IMG_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Freeze all the layers\n",
    "for layer in loaded_vgg.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# If I write here layer.trainable = True then layer from top will be true. WHY?? because the indexing of the array is taking this decision. fro  top\n",
    "# to before the last 4 layer I have told to be False rest are will be true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trainable status of the individual layers\n",
    "for layer in loaded_vgg.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(loaded_vgg)\n",
    "\n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    # Histogram normalization in y\n",
    "    hsv = color.rgb2hsv(img)\n",
    "    hsv[:,:,2] = exposure.equalize_hist(hsv[:,:,2])\n",
    "    img = color.hsv2rgb(hsv)\n",
    "\n",
    "    # central scrop\n",
    "    min_side = min(img.shape[:-1])\n",
    "    centre = img.shape[0]//2, img.shape[1]//2\n",
    "    img = img[centre[0]-min_side//2:centre[0]+min_side//2,centre[1]-min_side//2:centre[1]+min_side//2,:]\n",
    "#    img = rgb2gray(img)\n",
    "\n",
    "    # rescale to standard size\n",
    "    img = transform.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # roll color axis to axis 0\n",
    "#     img = np.rollaxis(img,-1) # this lin is doing the channel fisrt operation\n",
    "\n",
    "    return img\n",
    "\n",
    "def get_class(img_path):\n",
    "    return int(img_path.split('/')[-2])\n",
    "#     return str(img_path.split('/')[-2]) # returning the folder name. If use -1 that means image name. consider the img_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs = []\n",
    "labels = []\n",
    "root_dir = '/home/atif/machine_learning_stuff/ml_image/train_image_AI/'\n",
    "#path='/home/atif/training_by_several_learning_process/flower_photos/00000/'\n",
    "\n",
    "#all_img_paths = glob.glob(path+ '5547758_eea9edfd54_n_000.jpg')\n",
    "\n",
    "all_img_paths = glob.glob(os.path.join(root_dir, '*/*.png')) #I have done the training with .png format image. If another type of image will come \n",
    "                                                                                    #them .png will be changed by that extension\n",
    "np.random.shuffle(all_img_paths)\n",
    "for img_path in all_img_paths:\n",
    "    try:\n",
    "        img = preprocess_img(io.imread(img_path))\n",
    "        label = get_class(img_path)\n",
    "        imgs.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "        if len(imgs)%1200 == 0: print(\"Processed {}/{}\".format(len(imgs), len(all_img_paths)))\n",
    "            #print(\"get it 2\")\n",
    "    except (IOError, OSError):\n",
    "        print('missed', img_path)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(imgs, dtype='float32') #Keeping the image as an array\n",
    "X = X.reshape(len(imgs),IMG_SIZE,IMG_SIZE,IMG_depth) # write (IMG_SIZE,IMG_SIZE,1 if you want channel last; 1= grayscale;3=RGB)\n",
    "# Y = np.eye(NUM_CLASSES, dtype='uint8')[labels]\n",
    "Y = keras.utils.to_categorical(labels, NUM_CLASSES)\n",
    "\n",
    "print('X shape: ', X.shape,' type: ',type(X))\n",
    "print('Y shape: ', Y.shape,' type: ',type(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=sgd,\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/atif/machine_learning_stuff/model_file_keras/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"current time:\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return lr * (0.1 ** int(epoch / 10))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "do_train_model=model.fit(X, Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,verbose=2,\n",
    "          #np.resize(img, (-1, <image shape>)\n",
    "          callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "# ,ModelCheckpoint(path+str(current_time)+'_new_model_'+str(epochs)+'.h5', save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path+str(current_time)+'_vgg16_epoch_'+str(epochs)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training. No image augmentation\n",
    "# Image processing followed here different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2\n",
    "saved_model_path = '/home/atif/machine_learning_stuff/unreal_cv_image/unreal_cv_image_manipulation/classification_UE4_image/trained_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = IMG_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No Data augmentation \n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 100\n",
    "val_batchsize = 10\n",
    "\n",
    "# Data Generator for Training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Data Generator for Validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=epoch,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)\n",
    "\n",
    "# Save the Model\n",
    "model.save(saved_model_path+'all_freezed_old_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.savefig(saved_model_path+'vgg_16_img_aug_accuracy.jpg')\n",
    "# plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.savefig(saved_model_path+'vgg_16_img_aug_loss.jpg')\n",
    "# plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# model = load_model('/home/atif/machine_learning_stuff/model_file_keras/general_2_sep_ep_30_epoch.h5')\n",
    "model = load_model(path+'2019-09-20 00:57:57_vgg16_epoch_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('/home/atif/machine_learning_stuff/ml_image/test_image_keras_IAI.csv',sep=';')\n",
    "# test_image_path =  '/home/atif/machine_learning_stuff/ml_image/test_image_crop/'\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "i = 0\n",
    "for file_name, class_id  in zip(list(test['Filename']), list(test['ClassId'])):\n",
    "    print('filename: ',file_name,'\\t classId: ',class_id)\n",
    "    #print('classId: ',class_id)\n",
    "    img_path = os.path.join('/home/atif/machine_learning_stuff/ml_image/test_image_26_aug/',file_name)\n",
    "#     img_path = glob.glob(os.path.join(test_image_path, '*/*.jpg'))\n",
    "    X_test.append(preprocess_img(io.imread(img_path)))\n",
    "    y_test.append(class_id)\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "acc = np.sum(y_pred==y_test)/np.size(y_pred)\n",
    "print(\"Test accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "test_image_path = r'/home/atif/machine_learning_stuff/ml_image/test_image_26_aug/'\n",
    "\n",
    "my_name = ['SM_CalgonitFinish_2','SM_CalgonitFinishKlarspueler_5','SM_CalgonitFinishMaschinenpfleger_8','SM_CalgonitFinishSpezialSalz_11',\n",
    "           'SM_CalgonitFinishVorratspack_14','SM_DenkMitEdelstahlreiniger_17','SM_DenkMitEdelstahlReinigerSpray_20','SM_DenkMitEntkalker_23',\n",
    "           'SM_DenkMitGeschirrReiniger_26','SM_DenkMitGeschirrReinigerClassic_29','SM_DenkMitGeschirrReinigerEvo_32','SM_DenkMitGeschirrReinigerNature_35',\n",
    "           'SM_DenkMitHygieneAllzweckreiniger_38','SM_DenkMitMaschinenpfleger_41','SM_DenkMitSpezialsalz_44','SM_SomatClassic_53']\n",
    "\n",
    "img_path = glob.glob(test_image_path+ '/*.jpg')\n",
    "for image in img_path:\n",
    "    X_test=[]\n",
    "    X_test.append(preprocess_img(io.imread(image)))\n",
    "    X_test = np.array(X_test)\n",
    "#     plt.imshow(X_test)\n",
    "    X_test = X_test.reshape(len(X_test),IMG_SIZE,IMG_SIZE,3)\n",
    "    \n",
    "    print('image name is: ',image)\n",
    "    predicted_class = model.predict_classes(X_test)\n",
    "    print(\"predicted class: \",predicted_class)\n",
    "#     print('type-of predicted calss: ', type(predicted_class))\n",
    "    print('class name is: ',my_name[predicted_class[0]])\n",
    "    \n",
    "#     probability = model.predict_proba(X_test)\n",
    "#     print(\"probability: \",probability)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "results = confusion_matrix(y_test, y_pred)\n",
    "print(results)\n",
    "print(type(results))\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(results, range(16),\n",
    "                  range(16))\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.tight_layout()\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "\n",
    "ax = sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 14},xticklabels=True, yticklabels=True)# font size\n",
    "ax.set(xlabel='predicted_class', ylabel='true_class')\n",
    "ax.set_ylim(16)\n",
    "figure = ax.get_figure()    \n",
    "# figure.savefig('2_sep_ax_conf.png', dpi=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generator for prediction\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Get the filenames from the generator\n",
    "fnames = validation_generator.filenames\n",
    "\n",
    "# Get the ground truth from generator\n",
    "ground_truth = validation_generator.classes\n",
    "\n",
    "# Get the label to class mapping from the generator\n",
    "label2index = validation_generator.class_indices\n",
    "\n",
    "# Getting the mapping from class index to class label\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    "\n",
    "# Get the predictions from the model using the generator\n",
    "predictions = model.predict_generator(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)\n",
    "predicted_classes = np.argmax(predictions,axis=1)\n",
    "\n",
    "errors = np.where(predicted_classes != ground_truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))\n",
    "\n",
    "# Show the errors\n",
    "for i in range(len(errors)):\n",
    "    pred_class = np.argmax(predictions[errors[i]])\n",
    "    pred_label = idx2label[pred_class]\n",
    "    \n",
    "    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
    "        fnames[errors[i]].split('/')[0],\n",
    "        pred_label,\n",
    "        predictions[errors[i]][pred_class])\n",
    "    \n",
    "    original = load_img('{}/{}'.format(validation_dir,fnames[errors[i]]))\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.imshow(original)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
